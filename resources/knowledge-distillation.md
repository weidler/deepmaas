## Knowledge Distillation

**Legend**  
:baby: Abstracted Quick/Easy Information  
:mortar_board: Advanced/In-Depth Material

#### Papers

* :mortar_board: [Original Paper](https://arxiv.org/pdf/1503.02531.pdf) by Hinton et al. (2015).

##### Relevant Related Work

* [Model compression](http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf). Bucilua et al. (2006)
* [Learning small-size DNN with output-distribution-based criteria](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/zhao.pdf) Li et al. (2014)
  * They do the same but with different parameters

#### Articles


#### Videos
