## Knowledge Distillation

**Legend**  
:baby: Abstracted Quick/Easy Information  
:mortar_board: Advanced/In-Depth Material

#### Papers

* :mortar_board: [Original Paper](https://arxiv.org/pdf/1503.02531.pdf) by Hinton et al. (2015).
* :mortar_board: [SOTA on ImageNet](https://arxiv.org/abs/1911.04252) by Google Research
* :mortar_board: [Combination with the self-supervised paradigm](https://arxiv.org/pdf/2006.10029.pdf) by Google Research
* :mortar_board: [Attention Distillation](https://arxiv.org/abs/1612.03928) from ICLR


##### Relevant Related Work

* [Model compression](http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf). Bucilua et al. (2006)
* [Learning small-size DNN with output-distribution-based criteria](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/zhao.pdf) Li et al. (2014)
  * They do the same but with different parameters

#### Articles
* :baby: [Simple explanation of the ideas behind KD](https://ramesharvind.github.io/posts/deep-learning/knowledge-distillation/)

#### Videos
* :baby: [Simple implementation of KD for vision in TF/keras](https://youtu.be/Y2K13XDqwiM) by Henry AI Labs
